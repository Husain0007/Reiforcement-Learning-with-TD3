{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybullet in c:\\users\\home\\anaconda3\\lib\\site-packages (2.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pybullet_envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialze the Experience Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first episode we will populate the Experience Replay Memory with random actions.  \n",
    "Then in each subsequent training episode we will leverage some instances of the Experience Replay Memory to account for anomolous (state, action) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "    def add(self, transition):\n",
    "        if len (self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1)% self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size = batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, \\\n",
    "        batch_rewards, batch_dones = [], [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(done, copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states),\\\n",
    "        np.array(batch_actions), np.array(batch_rewards).reshape(-1,1),\\\n",
    "        np.array(batch_dones).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Neural Network for Actor Model & Actor Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Actor Class is created for easy instantiation during Training.\n",
    "To ensure we have a continuous action-space, as opposed to a discrete action space, we map the result of the *tanh() function* to the maximum allowed action-value, ie;  **self.max_action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__() # To activate inheritence\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400,300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x): ##Forward Propagation\n",
    "        x = F.relu(self.layer_1(x))    # F = Functional Module\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action *torch.tanh(self.layer_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Neural Network for Critic Models & Critic Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the second forward method to obtain the gradient ascent forward propagation of the first critic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        #Defining the first Critic Neural Network\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "\n",
    "        #Defining the second Critic Neural Network\n",
    "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_5 = nn.Linear(400, 300)\n",
    "        self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        # Forward-Propagation on the first Critic Neural Network\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        # Forward-Propagation on the second Critic Neural Network\n",
    "        x2 = F.relu(self.layer_4(xu))\n",
    "        x2 = F.relu(self.layer_5(x2))\n",
    "        x2 = self.layer_6(x2)\n",
    "        return x1, x2\n",
    "    \n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x))\n",
    "        x1 = self.layer_3(x)\n",
    "        return x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a full episode with first 10,000 actions played randomly to facilitate exploration. Then play actions played by the Actor model. Only after that do we start to sample from the Experience Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Training Process Class\n",
    "class TD3(object):\n",
    "    def __init__(self, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
